---
title: "Density Graphs"
subtitle: "Visualizing Full Distributions"
author:
  - Slides by Sam Rosen
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    # logo: images/quarto.png
    theme: theme.scss
---

## Accompanying interactive repo

## Distributions {.smaller}

There are many properties of a distribution of values

-   **Center**: Mean, Median, Modes
-   **Spread**: Variance, Range (Support), Interquartile range
-   **Shape**: Skewness, Kurtosis, Quantiles
-   Any statistic you can think of

Ultimately when analyzing data, the distribution is important to know how to proceed:

-   Parametric tests
-   Erratic Data
-   Outliers

So let's visualize them!

## Histograms {.smaller fig-align="center"}

```{r}
#| label: load-packages
library(tidyverse)
```

Histogram of 1000 random numbers generated from a $\textsf{Normal}(\mu=-1, \sigma=0.5)$ and 2000 generated from a $\textsf{Normal}(\mu=2, \sigma=0.75)$:

```{r}
#| fig-width: 10
#| fig-height: 4.5
#| fig-align: center

example_data <- data.frame(random_values=c(rnorm(200, -1, 0.5), rnorm(n=400, 2, 0.75)))
ggplot(example_data, aes(random_values)) +
  geom_histogram(binwidth = 0.1) +
  ggtitle("Histogram of two random normal variables") +
  xlim(-2.5, 5)
```

## Density Plots {.smaller}

**What's the difference?**

-   Histograms are *counts* of bins of *observed* data.
-   Density plots are *estimates* of the *unknown* distribution.

```{r}
#| fig-width: 10
#| fig-height: 4.5
#| fig-align: center

ggplot(example_data, aes(random_values)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, alpha=0.25) +
  geom_density(fill=4, alpha=0.50) +
  ggtitle("Density plot of two random normal variables") +
  xlim(-2.5, 5)
```

## So what? {.smaller .no-list-margins}

::: {.no-list-margins style="font-size:0.75em;"}
-   Histograms are sensitive to where the bins are cut

-   Histograms vary more per random sample than density plots

-   Density graphs are estimates for what a very fine histogram with lots of data would show
:::

```{ojs}
//| panel: sidebar

viewof binWidth = Inputs.range([0.01, 1.5], {value: 0.2, step: 0.001, label: "Bin Width"});
viewof numPoints = Inputs.range([0, 9000], {value: 900, step: 1, label: "Number of Points"});
viewof generate = Inputs.button("Regenerate Data");
```

```{ojs}
//| panel: fill
import { plotNewData, regenerateData } from "./histoSampling.js";

// Generate does not actually get used just forces a refresh
plotNewData(regenerateData(numPoints / 3), binWidth, generate);

```

::: footer
Based on [example](https://observablehq.com/@d3/kernel-density-estimation) by Mike Bostock
:::

## Motivating Example {.smallest}

-   Baseball! A home run in baseball occurs when a player hits a fair ball outside of the playing field. Examples:

::: {style="text-align: center"}
<iframe width="560" height="315" src="https://www.youtube.com/embed/nxuTG5SEHn8">

</iframe>
:::

-   Home runs are exciting! Baseball currently has a marketing problem, but throughout history Major League Baseball (MLB, the organization running the highest level of professional baseball) has tried to change the rules to increase home runs to help the game be more entertaining.

    -   In short terms, **Home runs = Money**, but if everyone hits the same number of home runs they become less exciting.
    -   Examining the distribution of home runs year-by-year we may be able to see the effect of rule changes.

## Data {.smallest}

<div>

```{r}
#| echo: TRUE

library(Lahman)
colnames(Batting)
```

Our dataset comes from the R package `Lahman`. Each row in the data frame is the hitting stats of a player for a given year. We will mostly be using the following columns:

| Variable | Description                                                                        |
|--------------------------|----------------------------------------------|
| `yearID` | The year for the statistics                                                        |
| `HR`     | The number of home runs a player hit in a given year                               |
| `RBI`    | Runs batted in, the number of runs (or points) a player achieved in a given year   |
| `SB`     | Stolen bases; more stolen bases = faster player                                    |
| `G`      | Number of games played; there are 162 games in a baseball season (154 before 1961) |
| `BB`     | Walks; more walks = defense is worried about player hitting home runs              |
| `SO`     | Strike outs; more strikeouts = Hitter is swinging recklessly                       |

</div>

## Data we will use {auto-animate="true"}

```{r}
#| echo: True
#| eval: False
home_runs <- Batting
```

We are interested in the distribution of the number of home runs individual players have hit per year. [^1]

[^1]: The record for number of home runs in one year is 73, by Barry Bonds in 2001. The baseball was sold for **\$517,000**!

## Context 1 {auto-animate="true"}

```{r}
#| echo: True
#| eval: False
home_runs <- Batting |> 
  filter(G >= 100 | 
           (G >= 40 & yearID == 2020) | 
           (G >= 70 & yearID == 1994),
         )
```

::: r-fit-text
-   There are many players in the dataset that played very little games per year, so we will limit to players that played at least 100 games in a given year, with the following years excepted:

    -   In 1994 only about 115 games were played due to labor strikes, so will filter to at least 70 games.

    -   In 2020 COVID shortened the season to only 60 games, so we will filter at least 40 games.
:::

## Context 2 {auto-animate="true"}

```{r}
#| echo: True
#| eval: False
home_runs <- Batting |> 
  filter(G >= 100 | 
           (G >= 40 & yearID == 2020) | 
           (G >= 70 & yearID == 1994),
         yearID > 1920,
         )
```

-   We are only concerned with years after 1920 (known as the "live-ball era").
-   Very few home runs were hit before 1920 as the same baseball was used for the entire game. About 100 baseballs are used every game today!

## Context 3 {auto-animate="true"}

```{r}
#| echo: True
#| eval: True
home_runs <- Batting |> 
  filter(G >= 100 | 
           (G >= 40 & yearID == 2020) | 
           (G >= 70 & yearID == 1994),
         yearID > 1920,
         lgID %in% c("AL", "NL"))
```

-   We are only considering the `AL` and `NL` leagues as they have the best stat-tracking and are the only Major leagues still around today.

## Density Graph Example {.smallest}

::: panel-tabset
## Plot

```{r}
#| fig-height: 4
ggplot(home_runs, aes(HR)) +
  geom_density() + 
  xlab("Home runs per player per year")
```

-   Most players hit just a few home runs per year and the distribution is very right-skewed.
-   Very few players hit more than 40 per year.

## Code

```{r}
#| eval: False
#| echo: True

ggplot(home_runs, aes(HR)) +
  geom_density() + 
  xlab("Home runs per player per year")
```
:::

## Stacked Density Graph By Decade {.smallest}

::: panel-tabset
## Plot

```{r}
#| fig-height: 4


home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(HR, fill = decade)) +
  geom_density(position = "stack") +
  xlab("Home runs per player per year")
```

-   If we stratify by decade, we can see the **mode** of the density graphs slowly creep forward, but it is difficult to see the tail of the distribution.

## Code

```{r}
#| eval: False
#| echo: True
home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(HR, color = decade)) +
  geom_density() +
  xlab("Home runs per player per year")
```
:::

## Overlapping Density Graphs By Decade {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4

home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(HR, color = decade)) +
  geom_density() +
  xlab("Home runs per player per year")
```

-   The modes moving forward is a little more apparent now, but the graphs are too coupled to digest easily.

### Code

```{r}
#| eval: False
#| echo: True
home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(HR, color = decade)) +
  geom_density() +
  xlab("Home runs per player per year")
```
:::

## Stacked Density Graph with Conditional Probabilities {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4

home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(x = HR, y = after_stat(count), fill = decade)) +
  geom_density(position = "fill") +
  geom_vline(xintercept = 60, linetype = 2) +
  xlab("Home runs per player per year")

```

-   By using `position="fill"` and `y=after_stat(count)` we graph the conditional probability that of a decade given a player has hit a certain number of home runs.
-   We see that players would hit about 60 homeruns in the 20's and 30's, but that disappears until the 90's and 2000's [^2] which quickly take up the conditional probability for years with more than 60 home runs. What happened?

### Code

```{r}
#| echo: True
#| eval: False

home_runs |>
  mutate(decade = cut(
    yearID,
    breaks = seq(1920, 2030, 10),
    labels = paste0(seq(1920, 2020, 10),
                    "'s")
  )) |>
  ggplot(aes(x = HR, y = after_stat(count), fill = decade)) +
  geom_density(position = "fill") +
  geom_vline(xintercept = 60, linetype = 2) +
  xlab("Home runs per player per year")
```
:::

[^2]: With the exception of 1961, the year Roger Maris hit 61 home runs.

## Violin Plot {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4

home_runs |>
  filter(yearID %in% 1985:2005) |>
  ggplot(aes(HR, x = factor(yearID))) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_jitter(
    data = ~ filter(.x, HR >= 30),
    height = 0,
    width = 0.1,
    alpha = 0.5
  ) +
  xlab("Home runs per player per year")
```

-   Let's examine the years near the change point, 1985 to 2005. All points shown are players that hit 30 or more homeruns in a given year. It looks like around 1995 players started hitting **a lot** more home runs.

### Code

```{r}
#| echo: True
#| eval: False
home_runs |>
  filter(yearID %in% 1985:2005) |>
  ggplot(aes(HR, x = factor(yearID))) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_jitter(
    data = ~ filter(.x, HR >= 30),
    height = 0,
    width = 0.1,
    alpha = 0.5
  ) +
  xlab("Home runs per player per year")

```
:::

## Ridge Plot {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4

library(ggridges)

home_runs |>
  filter(yearID %in% 1985:2010) |>
  ggplot(aes(x = HR,
             y = factor(yearID))) +
  stat_density_ridges(
    mapping = aes(fill = factor(stat(quantile))),
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = c(.25, .50, .75, .95),
    quantile_lines = TRUE,
    scale = 2,
    rel_min_height = 0.01
  ) +
  scale_fill_viridis_d(name = "Quantiles",
                       labels = c("0", "25th", "50th", "75th", "95th")) +
  geom_jitter(
    data = ~ filter(.x, HR >= 30),
    height = 0.2,
    width = 0,
    alpha = 0.3,
  ) +
  xlab("Home runs per player per year") +
  ylab("Year") +
  xlim(c(0, 73))

```

-   The quantiles also have a consistent increase, along with many more players hitting 30 or more home runs!
-   In 1998 there was a home run record race between two players; this brought a lot of interest back into baseball.
-   1995 to about 2005 is known as the *Steroid Era* in baseball. During this time, players would take performance enhancing drugs freely as the league did not enforce the ban on them. League-wide testing began in 2003.

### Code

```{r}
#| echo: True
#| eval: False

library(ggridges)

home_runs |>
  filter(yearID %in% 1985:2010) |>
  ggplot(aes(x = HR,
             y = factor(yearID))) +
  stat_density_ridges(
    mapping = aes(fill = factor(stat(quantile))),
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = c(.25, .50, .75, .95),
    quantile_lines = TRUE,
    scale = 2,
    rel_min_height = 0.01
  ) +
  scale_fill_viridis_d(name = "Quantiles",
                       labels = c("0", "25th", "50th", "75th", "95th")) +
  geom_jitter(
    data = ~ filter(.x, HR >= 30),
    height = 0.2,
    width = 0,
    alpha = 0.3,
  ) +
  xlab("Home runs per player per year") +
  ylab("Year") +
  xlim(c(0, 73))

```
:::

## Bandwidth {.smaller}

-   Density graphs are sensitive to **bandwidth**, but this is a continuous degradation of performance.

```{ojs}
//| panel: sidebar

viewof binWidth2 = Inputs.range([0.01, 1.5], {value: 0.2, step: 0.001, label: "Bin Width"});
viewof bandwidth = Inputs.range([0.01, 2], {value: 0.2, step: 0.001, label: "Bandwidth"});
viewof numPoints2 = Inputs.range([0, 9000], {value: 900, step: 1, label: "Number of Points"});
viewof generate2 = Inputs.button("Regenerate Data");
```

```{ojs}
//| panel: fill
import { plotNewDataBW } from "./bandwidthSampling.js";

// Generate does not actually get used just forces a refresh
plotNewDataBW(regenerateData(numPoints2 / 3), binWidth2, bandwidth, generate2);

```

## Automatic Bandwidth Selection {.smallest}

-   Because change in bandwidth leads to a continuous change in the density estimate, it is often easier to automatically pick a bandwidth!

-   Silverman's 'rule-of-thumb' `bw.nrd0` :

    $$ 
    \begin{align*}
      h = 0.9 * n^{-1/5} \min(s, IQR/1.34)
    \end{align*}
    $$

    -   One of the most optimal bandwidth selectors **if** your data comes from a normal distribution
    -   Default in `ggplot2` and `R`

-   Sheather-Jones `bw.SJ`

    -   More complicated bandwidth selector that "would rather fit" as the default

    -   Less likely to give over-smoothed density graphs

    -   `geom_density(bw="SJ")` to use

-   Other methods

    -   Scott's plug in estimator `bw.nrd`: similar to Silverman's

    -   `bw.ucv` and `bw.bcv`: cross validation based methods that are less useful for data visualization

    -   `bw.SJ( , method="dpi")`: An easier to calculate Sheather-Jones estimate that gives worse results

## Sheather Jones Example

::: panel-tabset
### Useful

```{r}
example_data <- data.frame(vals = c(rnorm(1000, 0, 1), rnorm(1000, 20, 1)))
true_density <- data.frame(x = c(seq(-4, 24, length.out = 10000)))
true_density <-
  true_density |> mutate(y = 0.5 * dnorm(x, 0, 1) + 0.5 * dnorm(x, 20, 1),
                         y_scaled = y / max(y))
ggplot(example_data, aes(x = vals)) +
  stat_density(aes(y = ..scaled.., color = "black"),
               geom = "line",
               bw = "SJ") +
  stat_density(aes(y = ..scaled.., color = "red"), geom = "line") +
  geom_line(
    data = true_density,
    mapping = aes(x = x,
                  y = y_scaled,
                  color = "blue",),
    linetype = "dashed"
  ) +
  scale_color_identity(
    name = "Bandwidth Estimator",
    breaks = c("black", "red", "blue"),
    labels = c("SJ", "Silverman's", "True Density"),
    guide = "legend"
  ) +
  labs(x = "Random Values", 
       y = "Scaled Density", 
       title = "Density of mixture of two random normals",
       subtitle = "Silverman's can over smooth even if generated from a Normal")

```

### Not useful? {.smallest}

```{r}

ggplot(home_runs, aes(x = HR)) +
  stat_density(aes(y = ..scaled.., color = "black"),
               geom = "line",
               bw = "SJ",
  ) +
  stat_density(aes(y = ..scaled.., color = "red"), geom = "line") +
  scale_color_identity(
    name = "Bandwidth Estimator",
    breaks = c("black", "red"),
    labels = c("SJ", "Silverman's"),
    guide = "legend"
  ) +
  labs(
    y = "Scaled Density",
    x = "Home runs per player per year",
    title = "Density of Home runs per person per year",
    subtitle = "Sheather Jones only puts density at integer values"
  )

```
:::

## Kernel Density Estimates (Advanced) {.smallest}

-   Density graphs are illustrations of Kernel Density Estimates:

$$
\begin{align*}
\hat f_h(x) & = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
\end{align*}
$$

-   $x_i$ is the $i^{th}$ data point

-   $h$ is the bandwidth of the Kernel

-   $K$ is the Kernel

    -   $K$ can be a number of functions (see `kernel` option from `?density` or [Wikipedia](https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use)) but is usually the Gaussian kernel: $K(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)$.
    -   Choice of $K$ will give different looking density graphs, but choice of bandwidth is **a lot** more important than choice of Kernel. The Gaussian Kernel is by far the most used.
    -   To see examples of Kernel choices, see this [shiny app](https://shinyserv.es/shiny/kde/) by Eduardo García-Portugués.

::: {.footer style="font-size: 0.5em;"}
To learn more, see [Chapter 2](https://bookdown.org/egarpor/NP-UC3M/kde-i-kde.html) of *Nonparametric Statistics* by Eduardo García-Portugués.
:::

## Cautions {.smallest}

::: panel-tabset
### Density Below 0

```{r}
#| echo: True
#| code-line-numbers: false
long_tailed_data <- data.frame(random_values = rlnorm(1000, -3, 1))
```

::: columns
::: {.column width="49%"}
```{r}
#| echo: True
# Base R plotting
plot(density(long_tailed_data$random_values, bw="SJ"), 
     main="Density graph of positive numbers with density below 0") 
abline(v = 0, col="red", lty = 2)
```
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
#| echo: True
plot(density(long_tailed_data$random_values, bw="SJ", 
             from = 0),
     main="Density graph of positive numbers with cut density")
abline(v = 0, col="red", lty = 2)
```
:::
:::

-   `ggplot2` generally handles this for you by putting bounds at the range of your data, but it can occasionally skip this depending on how complicated your graph becomes.

### Long Tailed Data

```{r}
#| echo: True
#| code-line-numbers: false
longer_tailed_data <- data.frame(random_values = rlnorm(1000, -6, 5))
```

::: columns
::: {.column width="49%"}
```{r}
#| echo: True

ggplot(longer_tailed_data, aes(random_values)) +
  geom_density()
```
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
#| echo: True
ggplot(longer_tailed_data, aes(random_values)) +
  geom_density() +
  scale_x_continuous(trans="log")
```
:::
:::

-   This occurs in practice quite often!
:::

## 2D Density {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 5

home_runs |>
  ggplot(aes(HR, SO + BB)) +
  geom_density_2d_filled() +
  geom_density_2d() +
  xlab("Home runs per player per year") +
  ylab("Strike outs and walks per player per year") +
  guides(fill = "none")

```

-   Players that hit lots of home runs tend to strikeout and walk more.

### Code

```{r}
#| eval: False
#| echo: True

home_runs |>
  ggplot(aes(HR, SO + BB)) +
  geom_density_2d_filled() +
  geom_density_2d() +
  xlab("Home runs per player per year") +
  ylab("Strike outs and walks per player per year") +
  guides(fill = "none")

```

### Scatter plot

```{r}
#| echo: True

home_runs |>
  ggplot(aes(HR, SO + BB)) +
  geom_jitter(width = 0.3, height = 0.3, alpha = 0.1) +
  geom_density_2d(alpha = 0.5) +
  xlab("Home runs per player per year") +
  ylab("Strike outs and walks per player per year")

```
:::

## Density Graphs Summary {.smaller}

Pros:

-   **Visualize entire distribution**
-   Mean, median, variance, outliers, support, skewness, normality etc.
-   `plot(density(Batting$HR))` is usually the first thing I do when analyzing data

Cons:

-   Sensitive to bandwidth choices
-   Harder to communicate to non-statisticians
-   Difficult to build yourself (use libraries!)

# Density Graphs 2

## Cumulative Distribution Functions (CDF) {.smaller}

-   For a random variable $X$, the CDF describes the probability that $X$ is below a certain value:

    -   Between 0 and 1 (like all probabilities)
    -   Non-decreasing
    -   Derivative is the PDF, i.e. the larger the PDF the faster the CDF is increasing.
    -   Example: If $X \sim \textsf{Normal}(0, 1)$

    $$
    \begin{align*}
      F_X(x) & = P(X \leq x) \\
      F_x(-\infty) & = 0 \\ 
      F_x(-1) & = 0.1587 \\ 
      F_x(0) & = 1/2 \\ 
      F_x(1) & =  0.8413 \\ 
      F_x(\infty) & = 1
    \end{align*}
    $$

## Empirical CDF (ECDF) {.smaller}

-   The empirical CDF of data is the proportion of data below a certain value:

    -   Between 0 and 1 (like all probabilities)
    -   Non-decreasing
    -   Increases at every value of observed data (step function)
    -   Example: `X = c(0, 1, 2, 2, 3, 3.5, 4)`

$$
\begin{align*}
  F_n(t) & = \frac{1}{n} \sum_{i=1}^n \begin{cases} 1 & x_i \leq t \\ 0 & \text{otherwise} \end{cases} \\
  F_7(-1) & = 0 \\
  F_7(0) & = 1/7 \\ 
  F_7(2.5) & = 4/7 \\
  F_7(4) & = 1 \\
  F_7(5) & = 1
\end{align*}
$$

## Empirical CDFs in R {.smaller}

```{r}
#| echo: True

new_example_data <- c(rnorm(n = 2000, mean = -1, sd = 0.5), 
                      rnorm(n = 4000, mean = 2, sd = 0.75))
mix_ecdf <- ecdf(new_example_data)
```

::: columns
::: {.column width="45%"}
```{r}
knitr::kable(data.frame(
  func_call = paste0("mix_ecdf(", c(-3, -1, 0, 2, 5), ")"),
  prob = mix_ecdf(c(-3, -1, 0, 2, 5))
),
digits = 4, format = "html", table.attr = "id=\"ecdf-table\"",
col.names = c("Function Call", "Probability less than value")
)

```
:::

::: {.column width="2%"}
:::

::: {.column width="45%"}
```{r}
as_df <- data.frame(vals = new_example_data, empirical_cdf = mix_ecdf(new_example_data))

ggplot(as_df, aes(x = vals)) +
  stat_density(aes(x = vals, y = ..scaled..), geom = "line", color = "green") +
  stat_ecdf(geom = "step", color = "red") +
  labs(x = "Data Value",
       y = "Density / Area Under Curve",
       title = "Comparison of Density and ECDF",
       subtitle = "The cumulative area under the density curve is the ECDF")


```
:::
:::

## Kolmogorov-Smirnov Test

-   **Theorem**: If two random quantities have equal CDF's they have the exact same distribution.
-   The *Kolmogorov-Smirnov Test* finds the maximum difference between two empirical CDF's and outputs a test statistic based on the sample sizes.
-   Assumes data is identically and independently distributed from a continuous distribution.
-   The test is approximate in the case of discrete data, but is good enough for our purposes.


## Comparing Distributions 1 {.smaller}


::: panel-tabset
### Output

::: columns
::: {.column width="50%"}
```{r}
#| echo: True
#| code-line-numbers: false

# Closest 4 years
HRearly2010s <- home_runs |> filter(yearID %in% 2011:2014)
# Years up to COVID season
HRlate2010s <- home_runs |> filter(yearID %in% 2016:2019) 

ks.test(HRearly2010s$HR, HRlate2010s$HR)
```
:::

::: {.column width="50%"}
```{r}
#| fig-height: 5
get_ks_df <- function(dat1, dat2) {
  ecdf1 <- ecdf(dat1)
  ecdf2 <- ecdf(dat2)
  grid_points <- seq(0, max(c(dat1, dat2)), length.out = 1000)
  differences <- abs(ecdf1(grid_points) - ecdf2(grid_points))
  ks_stat <- max(differences)
  first_max_location <-
    grid_points[which(differences == ks_stat)[1]]
  data.frame(
    x = first_max_location,
    xend = first_max_location,
    y = ecdf1(first_max_location),
    yend = ecdf2(first_max_location)
  )
}

ks_stat_2010s = get_ks_df(HRearly2010s$HR, HRlate2010s$HR)

ggplot(rbind(HRearly2010s, HRlate2010s), aes(HR, color = factor(yearID < 2015))) +
  stat_ecdf(geom = "step") +
  geom_segment(
    data = ks_stat_2010s,
    aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    color = "black",
    linetype = "dashed"
  ) +
  labs(x = "Homeruns per player per year",
       y = "Empirical CDF",
       title = "Empirical CDFs of player home runs per year in years 2011-2019 ",
       subtitle = "Dashed line is the Kolmogorov-Smirnov Statistic") +
  theme(legend.position = "bottom")

```
:::
:::


- Major League Baseball was accused of replacing the standard baseballs with "juiced" baseballs (easier to hit home runs) secretly in the middle of 2015. Is there credence to this claim?

### Code

```{r}
#| eval: False
#| echo: True

get_ks_df <- function(dat1, dat2) {
  ecdf1 <- ecdf(dat1)
  ecdf2 <- ecdf(dat2)
  grid_points <- seq(0, max(c(dat1, dat2)), length.out = 1000)
  differences <- abs(ecdf1(grid_points) - ecdf2(grid_points))
  ks_stat <- max(differences)
  first_max_location <-
    grid_points[which(differences == ks_stat)[1]]
  data.frame(
    x = first_max_location,
    xend = first_max_location,
    y = ecdf1(first_max_location),
    yend = ecdf2(first_max_location)
  )
}

ks_stat_2010s = get_ks_df(HRearly2010s$HR, HRlate2010s$HR)

ggplot(rbind(HRearly2010s, HRlate2010s), aes(HR, color = factor(yearID < 2015))) +
  stat_ecdf(geom = "step") +
  geom_segment(
    data = ks_stat_2010s,
    aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    color = "black",
    linetype = "dashed"
  ) +
  labs(x = "Homeruns per player per year",
       y = "Empirical CDF",
       title = "Empirical CDFs of player home runs per year in years 2011-2019 ",
       subtitle = "Dashed line is the Kolmogorov-Smirnov Statistic") +
  theme(legend.position = "bottom")

```

:::


## Comparing distributions 2 {.smaller}


::: panel-tabset
### Output

::: columns
::: {.column width="50%"}
```{r}
#| echo: True
#| code-line-numbers: false

HR2005 <- home_runs |> filter(yearID == 2005)
HR2006 <- home_runs |> filter(yearID == 2006)

ks.test(HR2005$HR, HR2006$HR)
```
:::

::: {.column width="50%"}
```{r}
#| fig-height: 5

ks_stat_0506 = get_ks_df(HR2005$HR, HR2006$HR)

ggplot(rbind(HR2005, HR2006), aes(HR, color = factor(yearID))) +
  stat_ecdf(geom = "step") +
  geom_segment(
    data = ks_stat_0506,
    aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    color = "black",
    linetype = "dashed"
  ) +
  labs(x = "Homeruns per player per year",
       y = "Empirical CDF",
       title = "Empirical CDFs of player home runs per year in years 2005 and 2006 ",
       subtitle = "Dashed line is the Kolmogorov-Smirnov Statistic",
       color = "Year") +
  theme(legend.position = "bottom")

```
:::
:::

- 2005 and 2006 are similar years in terms of home runs, so the Kolmorgov-Smirnov test does not reject.


### Code

```{r}
#| eval: False
#| echo: True
ks_stat_0506 = get_ks_df(HR2005$HR, HR2006$HR)

ggplot(rbind(HR2005, HR2006), aes(HR, color = factor(yearID))) +
  stat_ecdf(geom = "step") +
  geom_segment(
    data = ks_stat_0506,
    aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    color = "black",
    linetype = "dashed"
  ) +
  labs(x = "Homeruns per player per year",
       y = "Empirical CDF",
       title = "Empirical CDFs of player home runs per year in years 2005 and 2006 ",
       subtitle = "Dashed line is the Kolmogorov-Smirnov Statistic",
       color = "Year") +
  theme(legend.position = "bottom")
```

:::

## Using the Kolmogorov-Smirnov statistic for visualization

- Our earlier motivation was to compare the distribution of homeruns over time to see if rule changes made a difference.
- We can't use a ridge or violin plot with all 100 years; it would be too cramped.
- What if we used the KS statistic to compare all pairs of years and emulated a correlation matrix?

## Building the Matrix {.smallest}

```{r}
#| cache: TRUE
#| echo: TRUE
ks_matrix <- tribble(~year1, ~year2, ~ks_stat, ~p_value)
all_years <- unique(home_runs$yearID)

# Save some memory
home_runs_to_search <- home_runs |> select(yearID, HR)


options(warn = -1) # Turn off ks.test warning

for (year1 in all_years) {
  year1HR <- home_runs_to_search |> filter(yearID == year1)
  for (year2 in all_years) {
    if (year1 == year2) {
      next # Trivially 0 
    }
    year2HR = home_runs_to_search |> filter(yearID == year2)
    
    test <- ks.test(year1HR$HR,
                    year2HR$HR)
    
    ks_matrix <- ks_matrix %>%
      add_row(year1 = year1, 
              year2 = year2, 
              ks_stat = test$statistic, 
              p_value = test$p.value)
  }
}

options(warn = 0)
```






## Visualizing the Matrix {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4.5
#| fig-align: center

ks_matrix |>
  mutate(signif = cut(
    p_value,
    breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1.001),
    labels = c("0.001", "<0.01", "<0.05", "<0.1", "<1"),
    include.lowest = T,
  )) |>
  ggplot(aes(
    x = year1,
    y = year2,
    fill = factor(signif)
  )) +
  geom_tile() +
  scale_x_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_y_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_fill_manual(values = c(colorspace::heat_hcl(4), "#AAAAAA")) +
  labs(title = "Unadjusted p-values matrix",
       fill = "Significance",
       x = "Year",
       y = "Year") +
  coord_fixed()
```

- There seems to be some patterns in our matrix, but there's a problem...

### Code

```{r}
#| eval: False
#| echo: True

ks_matrix |>
  mutate(signif = cut(
    p_value,
    breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1.001),
    labels = c("0.001", "<0.01", "<0.05", "<0.1", "<1"),
    include.lowest = T,
  )) |>
  ggplot(aes(
    x = year1,
    y = year2,
    fill = factor(signif)
  )) +
  geom_tile() +
  scale_x_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_y_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_fill_manual(values = c(colorspace::heat_hcl(4), "#AAAAAA")) +
  labs(title = "Unadjusted p-values matrix",
       fill = "Significance",
       x = "Year",
       y = "Year") +
  coord_fixed()
```
:::


## Multiple Testing {.smallest}

- When performing multiple hypothesis tests, we typically want to control the *Family Wise Error Rate*: the probability we make at least 1 Type I error (a false rejection).

- Newer methods focus more on controlling the *False Discovery Rate*: the probability that any particular rejected null hypothesis is actually a false positive. 

- Both require adjusting p-values which is built in to `R` (`?p.adjust`).
  - `holm`, `hochberg` and `hommel` control for Family Wise Error Rate
    - `bonferroni` also controls for this, but is very conservative
  - `fdr` and `BY` methods control for False Discovery Rate

```{r}
#| echo: True

# Let's try all the adjustments and see how they change our visualization
ks_matrix <- ks_matrix |>
  mutate(
    p_holm = p.adjust(p_value, "holm"),
    p_hochberg = p.adjust(p_value, "hochberg"),
    p_hommel = p.adjust(p_value, "hommel"),
    p_bonferroni = p.adjust(p_value, "bonferroni"),
    p_fdr = p.adjust(p_value, "fdr"),
    p_BY = p.adjust(p_value, "BY")
  )
```

::: footer
To learn more see Chapter 13 of *An Introduction to Statistical Learning (2nd Edition)* by Hastie et al. It is [free online](https://www.statlearning.com/) and Chapter 13 is not too difficult. 
:::

## Visualizing the Matrix (Corrections) {.smallest}

::: panel-tabset
### Plot

```{r}
#| fig-height: 4.5
#| fig-align: center
ks_matrix |>
  pivot_longer(
    c(
      "p_holm",
      "p_hochberg",
      "p_hommel",
      "p_bonferroni",
      "p_fdr",
      "p_BY"
    ),
    names_to = "adjustment",
    values_to = "adjusted_p"
  ) |>
  mutate(signif = cut(
    adjusted_p,
    breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1.001),
    labels = c("0.001", "<0.01", "<0.05", "<0.1", "<1"),
    include.lowest = T,
  )) |>
  ggplot(aes(
    x = year1,
    y = year2,
    fill = factor(signif)
  )) +
  geom_tile() +
  scale_x_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_y_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_fill_manual(values = c(colorspace::heat_hcl(4), "#AAAAAA")) +
  labs(x = "Year",
       y = "Year",
       fill = "Significance") +
  facet_wrap( ~ adjustment) +
  coord_fixed() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
```

- Controlling for the Family Wise Error Rate got rid of a lot of our interesting patterns. We don't mind some false positives so we choose the `BY` adjustment as it controls False Discovery Rate but is a bit more conservative than `FDR`.

### Code

```{r}
#| eval: False
#| echo: True
ks_matrix |>
  pivot_longer(
    c(
      "p_holm",
      "p_hochberg",
      "p_hommel",
      "p_bonferroni",
      "p_fdr",
      "p_BY"
    ),
    names_to = "adjustment",
    values_to = "adjusted_p"
  ) |>
  mutate(signif = cut(
    adjusted_p,
    breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1.001),
    labels = c("0.001", "<0.01", "<0.05", "<0.1", "<1"),
    include.lowest = T,
  )) |>
  ggplot(aes(
    x = year1,
    y = year2,
    fill = factor(signif)
  )) +
  geom_tile() +
  scale_x_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_y_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_fill_manual(values = c(colorspace::heat_hcl(4), "#AAAAAA")) +
  labs(x = "Year",
       y = "Year",
       fill = "Significance") +
  facet_wrap( ~ adjustment) +
  coord_fixed() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
```
:::

## Annotating the Matrix (Events)

```{r}
mat_BY <- 
ks_matrix |>
  mutate(signif = cut(
    p_BY,
    breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1.001),
    labels = c("0.001", "<0.01", "<0.05", "<0.1", "<1"),
    include.lowest = T,
  )) |>
  ggplot(aes(x = year1,
             y = year2)) +
  geom_tile(aes(fill = factor(signif))) +
  scale_x_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_y_continuous(breaks = 1920 + seq(0, 10) * 10) +
  scale_fill_manual(values = c(colorspace::heat_hcl(4), "#AAAAAA")) +
  labs(title = "BY adjusted p-values matrix",
       fill = "Significance",
       x = "Year",
       y = "Year") +
  coord_fixed()


mat_BY +
  geom_tile(
    data = tribble(~ x, ~ y, ~ height, ~ width,
                   # Covid
                   2020, (1921 + 2021) / 2, 101, 1,
                  (1921 + 2021) / 2, 2020, 1, 101,
                    # YOTP
                   1968, (1921 + 2021) / 2, 101, 1,
                   (1921 + 2021) / 2, 1968, 1, 101,
                  # Strike
                   1994, (1921 + 2021) / 2, 101, 1,
                   (1921 + 2021) / 2, 1994, 1, 101,
                  # US in WW2
                   (1921 + 2021) / 2, 1943, 3, 101,
                   1943, (1921 + 2021) / 2, 101, 3
                    ),
    mapping = aes(
      x = x,
      y = y,
      height = height,
      width = width
    ),
    alpha = 0,
    color = "blue",
    size = 0.75
  )
```



